{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nIWV18rUnF-w"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ultralytics in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (8.3.143)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (1.24.3)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (3.7.2)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (1.10.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (2.4.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (5.9.0)\n",
            "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (2.0.3)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.0.5)\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from matplotlib>=3.3.0->ultralytics) (6.4.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (1.26.20)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from requests>=2.23.0->ultralytics) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.3.0->ultralytics) (3.20.2)\n",
            "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: torchmetrics in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torchmetrics) (1.24.3)\n",
            "Requirement already satisfied: packaging>17.1 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torchmetrics) (24.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torchmetrics) (2.4.1)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torchmetrics) (0.11.9)\n",
            "Requirement already satisfied: typing-extensions in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.10.0->torchmetrics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.10.0->torchmetrics) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from torch>=1.10.0->torchmetrics) (2025.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/yolov5/lib/python3.8/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "zsh:1: command not found: apt-get\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics  # 데이터 셋 분류 YOLO 방식\n",
        "!pip install torchmetrics # mAP 함수 구하는 메트릭스\n",
        "!apt-get -qq install fonts-nanum # 시각화시 한글 깨짐으로 추가한 나눔고딕체"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Canj7YGTnT40"
      },
      "source": [
        "### 라이브러리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Z78OSvw5qIef"
      },
      "outputs": [],
      "source": [
        "import matplotlib.font_manager as fm\n",
        "font_path = '/Users/jeonhyejeong/Library/Fonts/NanumGothic-Bold.ttf'\n",
        "prop = fm.FontProperties(fname=font_path, size=14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2zI7rKv6nE5M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from shutil import copy2\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "\n",
        "import cv2\n",
        "import random\n",
        "from ultralytics import YOLO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')\n",
        "import logging\n",
        "logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "MRXStKjgn04a"
      },
      "outputs": [],
      "source": [
        "# 경로 설정 (사용자 맞춤 절대 경로)\n",
        "ANNOTATIONS_DIR = Path(\"data/original/train_annotations\") # 원본\n",
        "IMAGES_DIR = Path(\"data/original/train_images\") # 원본\n",
        "OUTPUT_LABELS  = Path(\"data/labels\") # 수정한 레벨 데이터 경로\n",
        "OUTPUT_LABELS.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUTPUT_DIR = Path(\"data/project_rcnn\")\n",
        "IMAGE_TRAIN = OUTPUT_DIR / \"images\" / \"train\" # / img\n",
        "IMAGE_VAL   = OUTPUT_DIR / \"images\" / \"val\"\n",
        "LABEL_TRAIN = OUTPUT_DIR / \"labels\" / \"train\"\n",
        "LABEL_VAL   = OUTPUT_DIR / \"labels\" / \"val\"\n",
        "VAL_RATIO = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUDTsz_TnWKR"
      },
      "source": [
        "### 전처리\n",
        "데이터 분류<br>\n",
        "기존 코드의 전처리 결과 라벨 파일의 txt 내용이 알약 라벨링 , 바운딩박스 좌표 형태의 1줄 만 존재 <br>\n",
        "하나의 이미지에 여러 알약이 존재하여 1줄형태가 아닌 이미지에 해당하는 모든 정보 통합으로 변경"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "H7dmaiWMnscP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train images: 1192 | Val images: 297\n"
          ]
        }
      ],
      "source": [
        "for d in [IMAGE_TRAIN, IMAGE_VAL, LABEL_TRAIN, LABEL_VAL]:           # 라벨을 평가데이터와 학습 데이터로 나눔 8 : 2\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 전체 이미지 목록\n",
        "all_images = list(IMAGES_DIR.glob(\"*.png\"))\n",
        "random.shuffle(all_images)\n",
        "\n",
        "VAL_RATIO = 0.2  # 20% validation\n",
        "val_cnt = int(len(all_images) * VAL_RATIO)\n",
        "\n",
        "val_images = set(all_images[:val_cnt])\n",
        "train_images = set(all_images[val_cnt:])\n",
        "\n",
        "# 함수: 이미지와 라벨 쌍 복사\n",
        "def copy_dataset(image_list, img_dst_dir, label_dst_dir):\n",
        "    for img_path in image_list:\n",
        "        label_path = OUTPUT_LABELS / (img_path.stem + \".txt\")\n",
        "        copy2(img_path, img_dst_dir / img_path.name)\n",
        "        if label_path.exists():\n",
        "            copy2(label_path, label_dst_dir / label_path.name)\n",
        "        else:\n",
        "            # 라벨 없는 이미지면 빈 파일로 생성해도 됨 (선택)\n",
        "            open(label_dst_dir / label_path.name, 'w').close()\n",
        "\n",
        "copy_dataset(train_images, IMAGE_TRAIN, LABEL_TRAIN)\n",
        "copy_dataset(val_images, IMAGE_VAL, LABEL_VAL)\n",
        "\n",
        "print(f\"Train images: {len(train_images)} | Val images: {len(val_images)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MatMHTjNoibk"
      },
      "source": [
        "### 함수 및 변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uJotzAgsoft7"
      },
      "outputs": [],
      "source": [
        "id_to_index= {0: 0, 1899: 1, 2482: 2, 3350: 3, 3482: 4, 3543: 5, 3742: 6, 3831: 7, 4377: 8, 4542: 9, 5093: 10, 5885: 11, 6191: 12, 6562: 13, 10220: 14, 12080: 15, 12246: 16, 12419: 17, 12777: 18, 13394: 19, 13899: 20, 16231: 21, 16261: 22, 16547: 23, 16550: 24, 16687: 25, 18109: 26, 18146: 27, 18356: 28, 19231: 29, 19551: 30, 19606: 31, 19860: 32, 20013: 33, 20237: 34, 20876: 35, 21025: 36, 21324: 37, 21770: 38, 22073: 39, 22346: 40, 22361: 41, 22626: 42, 23202: 43, 23222: 44, 24849: 45, 25366: 46, 25437: 47, 25468: 48, 27652: 49, 27732: 50, 27776: 51, 27925: 52, 27992: 53, 28762: 54, 29344: 55, 29450: 56, 29666: 57, 29870: 58, 30307: 59, 31704: 60, 31862: 61, 31884: 62, 32309: 63, 33008: 64, 33207: 65, 33877: 66, 33879: 67, 34596: 68, 35205: 69, 36636: 70, 38161: 71, 41767: 72, 44198: 73}\n",
        "index_to_name= {0: 'background', 1: '보령부스파정 5mg', 2: '뮤테란캡슐 100mg', 3: '일양하이트린정 2mg', 4: '기넥신에프정(은행엽엑스)(수출용)', 5: '무코스타정(레바미피드)(비매품)', 6: '알드린정', 7: '뉴로메드정(옥시라세탐)', 8: '타이레놀정500mg', 9: '에어탈정(아세클로페낙)', 10: '삼남건조수산화알루미늄겔정', 11: '타이레놀이알서방정(아세트아미노펜)(수출용)', 12: '삐콤씨에프정 618.6mg/병', 13: '조인스정 200mg', 14: '쎄로켈정 100mg', 15: '리렉스펜정 300mg/PTP', 16: '아빌리파이정 10mg', 17: '자이프렉사정 2.5mg', 18: '다보타민큐정 10mg/병', 19: '써스펜8시간이알서방정 650mg', 20: '에빅사정(메만틴염산염)(비매품)', 21: '리피토정 20mg', 22: '크레스토정 20mg', 23: '가바토파정 100mg', 24: '동아가바펜틴정 800mg', 25: '오마코연질캡슐(오메가-3-산에틸에스테르90)', 26: '란스톤엘에프디티정 30mg', 27: '리리카캡슐 150mg', 28: '종근당글리아티린연질캡슐(콜린알포세레이트)\\xa0', 29: '콜리네이트연질캡슐 400mg', 30: '트루비타정 60mg/병', 31: '스토가정 10mg', 32: '노바스크정 5mg', 33: '마도파정', 34: '플라빅스정 75mg', 35: '엑스포지정 5/160mg', 36: '펠루비정(펠루비프로펜)', 37: '아토르바정 10mg', 38: '라비에트정 20mg', 39: '리피로우정 20mg', 40: '자누비아정 50mg', 41: '맥시부펜이알정 300mg', 42: '메가파워정 90mg/병', 43: '쿠에타핀정 25mg', 44: '비타비백정 100mg/병', 45: '놀텍정 10mg', 46: '자누메트정 50/850mg', 47: '큐시드정 31.5mg/PTP', 48: '아모잘탄정 5/100mg', 49: '세비카정 10/40mg', 50: '트윈스타정 40/5mg', 51: '카나브정 60mg', 52: '울트라셋이알서방정', 53: '졸로푸트정 100mg', 54: '트라젠타정(리나글립틴)', 55: '비모보정 500/20mg', 56: '레일라정', 57: '리바로정 4mg', 58: '렉사프로정 15mg', 59: '트라젠타듀오정 2.5/850mg', 60: '낙소졸정 500/20mg', 61: '아질렉트정(라사길린메실산염)', 62: '자누메트엑스알서방정 100/1000mg', 63: '글리아타민연질캡슐', 64: '신바로정', 65: '에스원엠프정 20mg', 66: '브린텔릭스정 20mg', 67: '글리틴정(콜린알포세레이트)', 68: '제미메트서방정 50/1000mg', 69: '아토젯정 10/40mg', 70: '로수젯정10/5밀리그램', 71: '로수바미브정 10/20mg', 72: '카발린캡슐 25mg', 73: '케이캡정 50mg', 74: \"넥시움정 40mg\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8YFRUWypofsm"
      },
      "outputs": [],
      "source": [
        "def show_multiple_predictions(dataset, model, class_names, num_images=12, score_threshold=0.5):  # 이미지 출력 코드 12개의 이미지로 num_images 입력값을 변경해서 이미지를 추가로 더뽑을 수 있음\n",
        "  idxs = random.sample(range(len(dataset)), k=num_images)\n",
        "  ncols = 3\n",
        "  nrows = 4\n",
        "  fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*5))\n",
        "\n",
        "  for ax, sample_idx in zip(axes.flatten(), idxs):\n",
        "      img, _ = dataset[sample_idx]\n",
        "      with torch.no_grad():\n",
        "          output = model([img.to(device)])[0]\n",
        "      npimg = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8).copy()\n",
        "      ax.imshow(npimg)\n",
        "      h, w = npimg.shape[:2]\n",
        "      boxes = output['boxes'].cpu().numpy()\n",
        "      labels = output['labels'].cpu().numpy()\n",
        "      scores = output['scores'].cpu().numpy()\n",
        "      for box, label, score in zip(boxes, labels, scores):\n",
        "          if score < score_threshold:\n",
        "              continue\n",
        "          x1, y1, x2, y2 = map(int, box)\n",
        "          rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2)\n",
        "          ax.add_patch(rect)\n",
        "          txt = f\"{class_names[label]}:{score:.2f}\"\n",
        "          ax.text(x1, y1-5, txt, fontsize=12, color='blue',\n",
        "                  fontproperties=prop,\n",
        "                  bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
        "      ax.set_axis_off()\n",
        "      ax.set_title(f\"샘플 {sample_idx}\", fontproperties=prop)\n",
        "  # 빈 ax가 남으면 지우기\n",
        "  for ax in axes.flatten()[len(idxs):]:\n",
        "      ax.remove()\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Vuc-htPSQ82n"
      },
      "outputs": [],
      "source": [
        "def show_test_predictions(dataset, model, class_names, device, num_images=12, score_threshold=0.5): # 테스트 이미지 시각화\n",
        "    idxs = random.sample(range(len(dataset)), k=min(num_images, len(dataset)))\n",
        "    ncols = 3\n",
        "    nrows = int(np.ceil(num_images / ncols))\n",
        "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*5, nrows*5))\n",
        "\n",
        "    axes = axes.flatten() if num_images > 1 else [axes]\n",
        "\n",
        "    for ax, sample_idx in zip(axes, idxs):\n",
        "        img, fname = dataset[sample_idx]\n",
        "        input_img = img.unsqueeze(0) if img.ndim == 3 else img\n",
        "        with torch.no_grad():\n",
        "            output = model(input_img.to(device))[0]\n",
        "        # img: (C, H, W)\n",
        "        npimg = (img.permute(1,2,0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        ax.imshow(npimg)\n",
        "        boxes = output['boxes'].cpu().numpy()\n",
        "        labels = output['labels'].cpu().numpy()\n",
        "        scores = output['scores'].cpu().numpy()\n",
        "        h, w = npimg.shape[:2]\n",
        "        for box, label, score in zip(boxes, labels, scores):\n",
        "            if score < score_threshold:\n",
        "                continue\n",
        "            x1, y1, x2, y2 = map(int, box)\n",
        "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='red', linewidth=2)\n",
        "            ax.add_patch(rect)\n",
        "            # 클래스명이 숫자 리스트인 경우 안전 처리\n",
        "            label_str = class_names[label] if label < len(class_names) else str(label)\n",
        "            txt = f\"{label_str}:{score:.2f}\"\n",
        "            ax.text(x1, max(y1-5, 0), txt, fontsize=12, color='blue',\n",
        "                    fontproperties=prop if prop else None,\n",
        "                    bbox=dict(facecolor='white', alpha=0.7, boxstyle='round'))\n",
        "        ax.set_axis_off()\n",
        "        ax.set_title(f\"{fname}\", fontproperties=prop if prop else None)\n",
        "    # 남은 빈 ax 지우기\n",
        "    for ax in axes[len(idxs):]:\n",
        "        ax.remove()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jd_5nVQNRFaU"
      },
      "outputs": [],
      "source": [
        "class LabelDataset(Dataset):                                # 라벨에 여러 데이터가 있는 경우의 데이터셋\n",
        "    def __init__(self, img_dir, label_dir, transforms=None):\n",
        "        self.img_paths = sorted(list(Path(img_dir).glob(\"*.png\")))\n",
        "        self.label_dir = Path(label_dir)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        img = cv2.imread(str(img_path))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # 라벨 읽기\n",
        "        label_path = self.label_dir / (img_path.stem + \".txt\")\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        if label_path.exists():\n",
        "            with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    splits = line.strip().split()\n",
        "                    if len(splits) != 5:\n",
        "                        continue\n",
        "                    class_id = int(splits[0])\n",
        "                    x_c, y_c, bw, bh = map(float, splits[1:])\n",
        "                    x1 = (x_c - bw/2) * w\n",
        "                    y1 = (y_c - bh/2) * h\n",
        "                    x2 = (x_c + bw/2) * w\n",
        "                    y2 = (y_c + bh/2) * h\n",
        "                    boxes.append([x1, y1, x2, y2])\n",
        "                    labels.append(class_id)\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4), dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        target = {\n",
        "            \"boxes\": boxes,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "        img = torch.from_numpy(img).permute(2,0,1).float() / 255.\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iiYHcSaKRISp"
      },
      "outputs": [],
      "source": [
        "class TestImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_list = [f for f in os.listdir(image_dir)\n",
        "                           if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_list[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, self.image_list[idx]  # (이미지, 파일명)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GQNHM0yo0Zb"
      },
      "source": [
        "### 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-kgZYWKGoyCZ"
      },
      "outputs": [],
      "source": [
        "# # Faster R-CNN 모델 생성 함수\n",
        "# def get_model_instance(num_classes: int):\n",
        "#     model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "#     model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "#     return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model_instance(num_classes: int):\n",
        "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "    # 모든 backbone 파라미터 freeze\n",
        "    for name, param in model.backbone.body.named_parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Layer4만 unfreeze\n",
        "    for name, param in model.backbone.body.layer4.named_parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # ROI head classifier 변경\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "LljqqQGpo1lY"
      },
      "outputs": [],
      "source": [
        "# 경로 및 클래스 수 지정\n",
        "train_img_dir = \"data/project_rcnn/images/train\"\n",
        "train_label_dir = \"data/project_rcnn/labels/train\"\n",
        "val_img_dir = \"data/project_rcnn/images/val\"\n",
        "val_label_dir = \"data/project_rcnn/labels/val\"\n",
        "test_img_dir = \"data/original/test_images\"                   # 원본테스트 이미지 경로\n",
        "\n",
        "# index_to_name은 {0:'background', 1:'알약1', ...} 형식\n",
        "num_classes = len(index_to_name)  # background 포함\n",
        "\n",
        "# 데이터셋, 로더\n",
        "train_dataset = LabelDataset(train_img_dir, train_label_dir)\n",
        "val_dataset = LabelDataset(val_img_dir, val_label_dir)\n",
        "test_dataset = TestImageDataset(test_img_dir, transform= transforms.ToTensor())\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NS6rgCuEo1iy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "model = get_model_instance(num_classes)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-3) # 파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "FMZlDczTo1dr"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x574137020>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5740dedc0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x574135090>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5bc148380>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5bda0c910>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5bdf9ed60>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x525ba27c0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5bdc5b540>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5ebb34960>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x526af8cd0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x525ba9a70>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5ebf259e0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x45c8d0bf0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n",
            "Error: command buffer exited with error status.\n",
            "\tThe Metal Performance Shaders operations encoded on it may not have completed.\n",
            "\tError: \n",
            "\t(null)\n",
            "\tInternal Error (0000000e:Internal Error)\n",
            "\t<AGXG13GFamilyCommandBuffer: 0x5b960a7f0>\n",
            "    label = <none> \n",
            "    device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "        name = Apple M1 \n",
            "    commandQueue = <AGXG13GFamilyCommandQueue: 0x17c3ff200>\n",
            "        label = <none> \n",
            "        device = <AGXG13GDevice: 0x17c3d4e00>\n",
            "            name = Apple M1 \n",
            "    retainedReferences = 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[32], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m images \u001b[38;5;241m=\u001b[39m [img\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m      9\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m---> 10\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:373\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    371\u001b[0m proposals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_coder\u001b[38;5;241m.\u001b[39mdecode(pred_bbox_deltas\u001b[38;5;241m.\u001b[39mdetach(), anchors)\n\u001b[1;32m    372\u001b[0m proposals \u001b[38;5;241m=\u001b[39m proposals\u001b[38;5;241m.\u001b[39mview(num_images, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m--> 373\u001b[0m boxes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_proposals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjectness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_anchors_per_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:280\u001b[0m, in \u001b[0;36mRegionProposalNetwork.filter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    277\u001b[0m boxes \u001b[38;5;241m=\u001b[39m box_ops\u001b[38;5;241m.\u001b[39mclip_boxes_to_image(boxes, img_shape)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# remove small boxes\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mbox_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_small_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m boxes, scores, lvl \u001b[38;5;241m=\u001b[39m boxes[keep], scores[keep], lvl[keep]\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# remove low scoring boxes\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# use >= for Backwards compatibility\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/envs/yolov5/lib/python3.8/site-packages/torchvision/ops/boxes.py:137\u001b[0m, in \u001b[0;36mremove_small_boxes\u001b[0;34m(boxes, min_size)\u001b[0m\n\u001b[1;32m    135\u001b[0m ws, hs \u001b[38;5;241m=\u001b[39m boxes[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m0\u001b[39m], boxes[:, \u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m boxes[:, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    136\u001b[0m keep \u001b[38;5;241m=\u001b[39m (ws \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_size) \u001b[38;5;241m&\u001b[39m (hs \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_size)\n\u001b[0;32m--> 137\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keep\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 30\n",
        "\n",
        "# 학습\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlVF1L7Lp1qo"
      },
      "outputs": [],
      "source": [
        "# 가중치 저장\n",
        "torch.save(model.state_dict(), \"fasterrcnn_model_R1.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFSHSqA9pxDY"
      },
      "outputs": [],
      "source": [
        "# 학습된 가중치 불러오기\n",
        "model.load_state_dict(torch.load(\"fasterrcnn_model_R1.pth\"))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iIGtZfUp5_h"
      },
      "source": [
        "#### 시각화 및 평가 지표 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfndcJ5zp5zZ"
      },
      "outputs": [],
      "source": [
        "# mAP 객체 생성 test 데이터는 라벨이 없어서 출력 불가능\n",
        "map_metric = MeanAveragePrecision(iou_type=\"bbox\")  # 기본은 COCO mAP@[.5:.95]\n",
        "\n",
        "model.eval()\n",
        "device = next(model.parameters()).device\n",
        "num_images = 200  # 평가할 이미지 개수 (전부 다 할 수도 있음)\n",
        "\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "for i in range(num_images):\n",
        "    img, target = val_dataset[i]\n",
        "    with torch.no_grad():\n",
        "        pred = model([img.to(device)])[0]\n",
        "    if len(target['labels']) == 0:   # GT 없음\n",
        "        continue                     # 평가에서 스킵\n",
        "    # 예측 박스\n",
        "    preds = {\n",
        "        \"boxes\": pred['boxes'].cpu(),\n",
        "        \"scores\": pred['scores'].cpu(),\n",
        "        \"labels\": pred['labels'].cpu()\n",
        "    }\n",
        "    # GT(정답) 박스\n",
        "    targets = {\n",
        "        \"boxes\": target['boxes'],\n",
        "        \"labels\": target['labels']\n",
        "    }\n",
        "    map_metric.update([preds], [targets])  # 리스트 형태여야 함\n",
        "\n",
        "# mAP, AP50 등 리포트\n",
        "result = map_metric.compute()\n",
        "print(\"mAP:\", result['map'].item()) # 50~95\n",
        "print(\"mAP@0.5:\", result['map_50'].item())\n",
        "print(\"mAP@0.75:\", result['map_75'].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxu202CCp5xO"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "class_names = [index_to_name[i] for i in range(len(index_to_name))]\n",
        "show_multiple_predictions(val_dataset, model, class_names, num_images=10, score_threshold=0.5) # val_data로 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uXSi2UTRejF"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "class_names = [index_to_name[i] for i in range(len(index_to_name))]\n",
        "show_test_predictions(test_dataset, model, class_names, device, num_images=12, score_threshold=0.5) # test_data로 시각화"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "yolov5Kernel",
      "language": "python",
      "name": "yolov5"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
